{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary modules\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an OpenAI account for later steps.\n",
    "1. https://platform.openai.com/signup\n",
    "2. https://platform.openai.com/account/api-keys\n",
    "3. Set your environment variable OPENAI_API_KEY to be equal to the key value from step 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the necessary path variables\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "#Define a prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 1582 chunks.\n",
      "Saved 1582 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "#Create the function to load our text to use for informing our LLM\n",
    "def generate_data_store():\n",
    "\n",
    "    #Load the documents\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.txt\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    #Split the text into smaller, manageable chunks\n",
    "    # This helps, when we search through the data, with the chunk being more\n",
    "    # useful and relevant to what answer we might be looking for\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, #Size in number of characters\n",
    "        chunk_overlap=100, #Overlap size in number of characters\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    #In order to query each chunk, we need to save them as a database\n",
    "    # We use ChromaDB for this, which uses vector embeddings as the key\n",
    "    #Save the output to Chroma\n",
    "    #Clear the database if one already exists\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    #Create a new database from our loaded documents, using OpenAI for the embeddings\n",
    "    # Save the database to disk to reuse later or save elsewhere (e.g. a lamba function) \n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n",
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "Ender remembered his own brother, and the memory was not fond.\n",
      "\n",
      "---\n",
      "\n",
      "\"While I killed billions.\"\n",
      "\n",
      "\"I wasn't going to say that.\"\n",
      "\n",
      "\"So he wanted to use me?\"\n",
      "\n",
      "\"He had plans for you, Ender. He would publicly reveal himself when you arrived, going to meet you in front of all the videos. Ender Wiggin's older brother, who also happened to be the great Locke, the architect of peace. Standing next to you, he would look quite mature. And the physical resemblance between you is stronger than ever. It would be quite simple for him, then, to take over.\"\n",
      "\n",
      "---\n",
      "\n",
      "this isn't Fairyland anymore. It's beyond the End of the World, and--\"\n",
      "\n",
      "\"I know the names of the places, I just don't know what ney mean.\"\n",
      "\n",
      "\"Fairyland was programmed in. It's mentioned in a few other places. But nothing talks\n",
      "\n",
      "about the End of the World. We don't have any experience with it.\"\n",
      "\n",
      "\"I don't like having the computer screw around with Ender's mind that way. Peter Wiggin is the most potent person in his life, except maybe his sister Valentine.\"\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is the name of Ender's sister?\n",
      "\n",
      "Response: Valentine\n",
      "Sources: ['data\\\\EndersGame.txt', 'data\\\\EndersGame.txt', 'data\\\\EndersGame.txt']\n"
     ]
    }
   ],
   "source": [
    "#Now we can query the database with our uploaded\n",
    "# data chunks to find a relevant response\n",
    "def query_data(query_text):\n",
    "    #Prepare the chroma database we created earlier\n",
    "    embedding_function = OpenAIEmbeddings()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    #Search the database for the chunk that best matches our query\n",
    "    # This will help ensure we get a relevant response\n",
    "    # k=3 means we retrieve the 3 best matches for our query\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "    #The above returns a list of tuples containing a document and its relevance score\n",
    "    # Here we ensure we find a match with a relevant of at least 0.7 before moving on\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Unable to find matching results.\")\n",
    "        return\n",
    "    \n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(prompt)\n",
    "\n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "\n",
    "query_data(\"What is the name of Ender's sister?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
