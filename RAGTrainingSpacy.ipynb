{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary modules\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the necessary path variables\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "#Define a prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate word embeddings using Spacy instead of OpenAI\n",
    "class SpacyEmbeddings:\n",
    "    \"\"\"\n",
    "    Class for generating Spacy-based embeddings for documents and queries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the SpacyEmbeddings object by loading the Spacy model.\n",
    "        \"\"\"\n",
    "        self.nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        Embed a list of documents using Spacy.\n",
    "\n",
    "        Args:\n",
    "            texts (list): A list of documents.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of document embeddings.\n",
    "        \"\"\"\n",
    "        return [self.nlp(text).vector.tolist() for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"\n",
    "        Embed a query using Spacy.\n",
    "\n",
    "        Args:\n",
    "            text (str): The query text.\n",
    "\n",
    "        Returns:\n",
    "            list: The query embedding.\n",
    "        \"\"\"\n",
    "        return self.nlp(text).vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 1582 chunks.\n",
      "Saved 1582 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "#Create the function to load our text to use for informing our LLM\n",
    "def generate_data_store():\n",
    "\n",
    "    #Load the documents\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.txt\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    #Split the text into smaller, manageable chunks\n",
    "    # This helps, when we search through the data, with the chunk being more\n",
    "    # useful and relevant to what answer we might be looking for\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, #Size in number of characters\n",
    "        chunk_overlap=100, #Overlap size in number of characters\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "\n",
    "    #In order to query each chunk, we need to save them as a database\n",
    "    # We use ChromaDB for this, which uses vector embeddings as the key\n",
    "    #Save the output to Chroma\n",
    "    #Clear the database if one already exists\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    #Get spacy embeddings\n",
    "    spacy_embeddings = SpacyEmbeddings()\n",
    "\n",
    "    #Create a new database from our loaded documents, using OpenAI for the embeddings\n",
    "    # Save the database to disk to reuse later or save elsewhere (e.g. a lamba function) \n",
    "    db = Chroma.from_documents(\n",
    "        chunks, spacy_embeddings, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "    return db\n",
    "\n",
    "db = generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BrendanHaas\\envs\\RAGLLM\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data\\\\EndersGame.txt', 'start_index': 490806}, page_content='\"What then? What happened to them? When they failed?\"\\n\\n\"Why does it matter, Ender?\"\\n\\nEnder didn\\'t answer.\\n\\n\"None of them failed at this point in their course, Ender. You made a mistake with Petra. She\\'ll recover. But Petra is Petra, and you are you.\"\\n\\n\"Part of what I am is her. Is what she made me.\"'), -580.582984231293), (Document(metadata={'source': 'data\\\\EndersGame.txt', 'start_index': 248793}, page_content='\"Are you helping Peter?\" asked Graff.\\n\\nShe didn\\'t answer.\\n\\n\"Is Peter such a very bad person, Valentine?\"\\n\\nShe nodded.\\n\\n\"Is Peter the worst person in the world?\"\\n\\n\"How can he be? I don\\'t know. He\\'s the worst person I know.\"\\n\\n\"And yet you and Ender are his brother and sister. You have the same genes, the same parents, how can he be so bad if--\"'), -619.4594423000128), (Document(metadata={'source': 'data\\\\EndersGame.txt', 'start_index': 13723}, page_content='This is all right, Ender thought. Talk and talk, Peter. Talk is fine.\\n\\n\"Well, now your guardian angels aren\\'t watching over you,\" Peter said. \"Now they aren\\'t checking to see if you feel pain, listening to hear what I\\'m saying, seeing what I\\'m doing to you. How about that? How about it?\"\\n\\nEnder shrugged.\\n\\nSuddenly Peter smiled and clapped his hands together in a mockery of good cheer. \"Let\\'s play buggers and astronauts,\" he said.\\n\\n\"Where\\'s Mom?\" asked Valentine.'), -626.7718517329962)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(metadata={'source': 'data\\\\EndersGame.txt', 'start_index': 490806}, page_content='\"What then? What happened to them? When they failed?\"\\n\\n\"Why does it matter, Ender?\"\\n\\nEnder didn\\'t answer.\\n\\n\"None of them failed at this point in their course, Ender. You made a mistake with Petra. She\\'ll recover. But Petra is Petra, and you are you.\"\\n\\n\"Part of what I am is her. Is what she made me.\"'), -580.582984231293), (Document(metadata={'source': 'data\\\\EndersGame.txt', 'start_index': 248793}, page_content='\"Are you helping Peter?\" asked Graff.\\n\\nShe didn\\'t answer.\\n\\n\"Is Peter such a very bad person, Valentine?\"\\n\\nShe nodded.\\n\\n\"Is Peter the worst person in the world?\"\\n\\n\"How can he be? I don\\'t know. He\\'s the worst person I know.\"\\n\\n\"And yet you and Ender are his brother and sister. You have the same genes, the same parents, how can he be so bad if--\"'), -619.4594423000128), (Document(metadata={'source': 'data\\\\EndersGame.txt', 'start_index': 13723}, page_content='This is all right, Ender thought. Talk and talk, Peter. Talk is fine.\\n\\n\"Well, now your guardian angels aren\\'t watching over you,\" Peter said. \"Now they aren\\'t checking to see if you feel pain, listening to hear what I\\'m saying, seeing what I\\'m doing to you. How about that? How about it?\"\\n\\nEnder shrugged.\\n\\nSuddenly Peter smiled and clapped his hands together in a mockery of good cheer. \"Let\\'s play buggers and astronauts,\" he said.\\n\\n\"Where\\'s Mom?\" asked Valentine.'), -626.7718517329962)]\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "\"What then? What happened to them? When they failed?\"\n",
      "\n",
      "\"Why does it matter, Ender?\"\n",
      "\n",
      "Ender didn't answer.\n",
      "\n",
      "\"None of them failed at this point in their course, Ender. You made a mistake with Petra. She'll recover. But Petra is Petra, and you are you.\"\n",
      "\n",
      "\"Part of what I am is her. Is what she made me.\"\n",
      "\n",
      "---\n",
      "\n",
      "\"Are you helping Peter?\" asked Graff.\n",
      "\n",
      "She didn't answer.\n",
      "\n",
      "\"Is Peter such a very bad person, Valentine?\"\n",
      "\n",
      "She nodded.\n",
      "\n",
      "\"Is Peter the worst person in the world?\"\n",
      "\n",
      "\"How can he be? I don't know. He's the worst person I know.\"\n",
      "\n",
      "\"And yet you and Ender are his brother and sister. You have the same genes, the same parents, how can he be so bad if--\"\n",
      "\n",
      "---\n",
      "\n",
      "This is all right, Ender thought. Talk and talk, Peter. Talk is fine.\n",
      "\n",
      "\"Well, now your guardian angels aren't watching over you,\" Peter said. \"Now they aren't checking to see if you feel pain, listening to hear what I'm saying, seeing what I'm doing to you. How about that? How about it?\"\n",
      "\n",
      "Ender shrugged.\n",
      "\n",
      "Suddenly Peter smiled and clapped his hands together in a mockery of good cheer. \"Let's play buggers and astronauts,\" he said.\n",
      "\n",
      "\"Where's Mom?\" asked Valentine.\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: Who is Ender Wiggin?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BrendanHaas\\envs\\RAGLLM\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ender Wiggin is a character in the story who is dealing with the influence and manipulation of his siblings, Peter and Valentine, as well as facing challenges and pressure in his training to potentially save the world from an alien threat.\n",
      "Sources: ['data\\\\EndersGame.txt', 'data\\\\EndersGame.txt', 'data\\\\EndersGame.txt']\n"
     ]
    }
   ],
   "source": [
    "#Now we can query the database with our uploaded\n",
    "# data chunks to find a relevant response\n",
    "def query_data(query_text):\n",
    "    #Prepare the chroma database we created earlier\n",
    "    embedding_function = SpacyEmbeddings()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    #Search the database for the chunk that best matches our query\n",
    "    # This will help ensure we get a relevant response\n",
    "    # k=3 means we retrieve the 3 best matches for our query\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "    print(results)\n",
    "\n",
    "    #The above returns a list of tuples containing a document and its relevance score\n",
    "    if len(results) == 0:\n",
    "        print(f\"Unable to find matching results.\")\n",
    "        return\n",
    "    \n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(prompt)\n",
    "\n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "\n",
    "query_data(\"Who is Ender Wiggin?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
